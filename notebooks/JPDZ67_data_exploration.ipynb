{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a1c149",
   "metadata": {},
   "source": [
    "# ANALYSIS OF THE COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0929d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f529ea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 18.8 ms, total: 200 ms\n",
      "Wall time: 198 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime                 object\n",
       "city                     object\n",
       "state                    object\n",
       "country                  object\n",
       "shape                    object\n",
       "duration (seconds)       object\n",
       "duration (hours/min)     object\n",
       "comments                 object\n",
       "date posted              object\n",
       "latitude                 object\n",
       "longitude               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#raw_data = pd.read_csv('./data/complete.csv')\n",
    "scrubbed_data = pd.read_csv('../raw_data/scrubbed.csv', low_memory=False)\n",
    "scrubbed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720c29bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration (seconds)</th>\n",
       "      <th>duration (hours/min)</th>\n",
       "      <th>comments</th>\n",
       "      <th>date posted</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/10/1949 20:30</td>\n",
       "      <td>san marcos</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>cylinder</td>\n",
       "      <td>2700</td>\n",
       "      <td>45 minutes</td>\n",
       "      <td>This event took place in early fall around 194...</td>\n",
       "      <td>4/27/2004</td>\n",
       "      <td>29.8830556</td>\n",
       "      <td>-97.941111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/10/1949 21:00</td>\n",
       "      <td>lackland afb</td>\n",
       "      <td>tx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>light</td>\n",
       "      <td>7200</td>\n",
       "      <td>1-2 hrs</td>\n",
       "      <td>1949 Lackland AFB&amp;#44 TX.  Lights racing acros...</td>\n",
       "      <td>12/16/2005</td>\n",
       "      <td>29.38421</td>\n",
       "      <td>-98.581082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/10/1955 17:00</td>\n",
       "      <td>chester (uk/england)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gb</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>20 seconds</td>\n",
       "      <td>Green/Orange circular disc over Chester&amp;#44 En...</td>\n",
       "      <td>1/21/2008</td>\n",
       "      <td>53.2</td>\n",
       "      <td>-2.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/10/1956 21:00</td>\n",
       "      <td>edna</td>\n",
       "      <td>tx</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>20</td>\n",
       "      <td>1/2 hour</td>\n",
       "      <td>My older brother and twin sister were leaving ...</td>\n",
       "      <td>1/17/2004</td>\n",
       "      <td>28.9783333</td>\n",
       "      <td>-96.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/10/1960 20:00</td>\n",
       "      <td>kaneohe</td>\n",
       "      <td>hi</td>\n",
       "      <td>us</td>\n",
       "      <td>light</td>\n",
       "      <td>900</td>\n",
       "      <td>15 minutes</td>\n",
       "      <td>AS a Marine 1st Lt. flying an FJ4B fighter/att...</td>\n",
       "      <td>1/22/2004</td>\n",
       "      <td>21.4180556</td>\n",
       "      <td>-157.803611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           datetime                  city state country     shape  \\\n",
       "0  10/10/1949 20:30            san marcos    tx      us  cylinder   \n",
       "1  10/10/1949 21:00          lackland afb    tx     NaN     light   \n",
       "2  10/10/1955 17:00  chester (uk/england)   NaN      gb    circle   \n",
       "3  10/10/1956 21:00                  edna    tx      us    circle   \n",
       "4  10/10/1960 20:00               kaneohe    hi      us     light   \n",
       "\n",
       "  duration (seconds) duration (hours/min)  \\\n",
       "0               2700           45 minutes   \n",
       "1               7200              1-2 hrs   \n",
       "2                 20           20 seconds   \n",
       "3                 20             1/2 hour   \n",
       "4                900           15 minutes   \n",
       "\n",
       "                                            comments date posted    latitude  \\\n",
       "0  This event took place in early fall around 194...   4/27/2004  29.8830556   \n",
       "1  1949 Lackland AFB&#44 TX.  Lights racing acros...  12/16/2005    29.38421   \n",
       "2  Green/Orange circular disc over Chester&#44 En...   1/21/2008        53.2   \n",
       "3  My older brother and twin sister were leaving ...   1/17/2004  28.9783333   \n",
       "4  AS a Marine 1st Lt. flying an FJ4B fighter/att...   1/22/2004  21.4180556   \n",
       "\n",
       "   longitude   \n",
       "0  -97.941111  \n",
       "1  -98.581082  \n",
       "2   -2.916667  \n",
       "3  -96.645833  \n",
       "4 -157.803611  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrubbed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bc1d8",
   "metadata": {},
   "source": [
    "## Let's identify topics mentioned in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75a34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "\n",
    "def clean (text:str):\n",
    "    \n",
    "    if isinstance(text,str):\n",
    "                         \n",
    "        for punctuation in string.punctuation:\n",
    "            text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "\n",
    "        lowercased = text.lower() # Lower Case\n",
    "        unaccented_string = unidecode.unidecode(lowercased) # remove accents\n",
    "        tokenized = word_tokenize(unaccented_string) # Tokenize\n",
    "#         words_only = [word for word in tokenized if word.isalpha()] # Remove numbers\n",
    "        words_only = [word for word in tokenized] # Keep numbers\n",
    "        stop_words = set(stopwords.words('english')) # Make stopword list\n",
    "        without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "\n",
    "        return without_stopwords\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 'No comments'\n",
    "    \n",
    "\n",
    "scrubbed_data['clean_comments'] = scrubbed_data['comments'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b0ee351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of rows with comments = 99.981%\n"
     ]
    }
   ],
   "source": [
    "# Percentage of comments\n",
    "\n",
    "no_comments_ = scrubbed_data[scrubbed_data['clean_comments'] == \"No comments\"].clean_comments.count()\n",
    "print(f\"Percentage of rows with comments = {round(1-(no_comments_/scrubbed_data.shape[0]),5)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7accbf19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [event, took, place, early, fall, around, 1949...\n",
       "1    [1949, lackland, afb, 44, tx, lights, racing, ...\n",
       "2    [green, orange, circular, disc, chester, 44, e...\n",
       "3    [older, brother, twin, sister, leaving, edna, ...\n",
       "4    [marine, 1st, lt, flying, fj4b, fighter, attac...\n",
       "Name: clean_comments, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrubbed_data['clean_comments'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db303994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_text(text:str):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in text]   \n",
    "    return ' '.join([w for w in lemmatized])\n",
    "\n",
    "scrubbed_data['lemmatize_comments'] = scrubbed_data['clean_comments'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193ffad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    event took place early fall around 1949 50 occ...\n",
       "1    1949 lackland afb 44 tx light racing across sk...\n",
       "2        green orange circular disc chester 44 england\n",
       "3    older brother twin sister leaving edna theater...\n",
       "4    marine 1st lt flying fj4b fighter attack aircr...\n",
       "Name: lemmatize_comments, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrubbed_data['lemmatize_comments'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62c6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e41c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS_ = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de913c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2, 3)).fit(scrubbed_data['lemmatize_comments'])\n",
    "data_vectorized = vectorizer.transform(scrubbed_data['lemmatize_comments'])\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=N_TOPICS_).fit(data_vectorized)\n",
    "lda_vectors = lda_model.transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc430a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for idx, topic in enumerate(lda_model.components_):\n",
    "        topic_dict[f\"topic_{idx}\"] = [(vectorizer.get_feature_names()[i], topic[i]) \\\n",
    "                                      for i in topic.argsort()[:-20 - 1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23faac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_list (topic_dict, index):\n",
    "    \n",
    "    list = []\n",
    "    \n",
    "    for i in range(len(topic_dict[index])):    \n",
    "        list.append(topic_dict[index][i][0])\n",
    "    \n",
    "    return \" | \".join(list)\n",
    "\n",
    "dict_ = {}\n",
    "\n",
    "for key in topic_dict.keys():    \n",
    "    dict_[key] = topic_list (topic_dict, key)\n",
    "    \n",
    "comments_df = pd.DataFrame.from_dict(dict_, orient='index').reset_index()\\\n",
    "                                                           .rename(columns={'index':'Topic',0:'Content'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ffe054",
   "metadata": {},
   "source": [
    "### Select main topic per sighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4317fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrubbed_data['main_topic'] = pd.DataFrame(lda_vectors).apply(lambda row: f\"topic_{row.argmax()}\", axis=1)\n",
    "\n",
    "for i in np.arange(0,N_TOPICS_):\n",
    "    scrubbed_data[f\"topic_{i}\"] = lda_vectors [:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrubbed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197590b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['datetime','city','lemmatize_comments','main_topic']\n",
    "scrubbed_comments_df_ = scrubbed_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22471bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrubbed_comments_df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd5bea",
   "metadata": {},
   "source": [
    "## Let's create 10 clusters of topics with 20 ngrams ( bigrams or trigrams ) each\n",
    "\n",
    "### > look at \"scrubbed_comments_topics.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad68102",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22296415",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('../raw_data/scrubbed_comments_topics.csv',header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
